{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"가영,호섭님 모델 참고.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WH4v2bDi8k9g"},"source":["# 공유 자전거 수요 예측 (11.30 고지혜)"]},{"cell_type":"markdown","metadata":{"id":"OiDstRzn8k9i"},"source":["### 라이브러리 및 데이터 불러오기"]},{"cell_type":"code","metadata":{"id":"aCEOpGxXtNDG"},"source":["# 기본 라이브러리\n","import numpy as np\n","import pandas as pd \n","\n","# 시각화 라이브러리\n","import matplotlib.pyplot as plt \n","import seaborn as sns\n","\n","# 모델링을 위한 sklearn 패키지\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from lightgbm import LGBMRegressor\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn import metrics\n","\n","# score를 내줄 함수\n","from sklearn.metrics import make_scorer\n","\n","# 모델링에 활용한 패키지\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MkRbbpH39LY5"},"source":["drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fcBY_dBK8k9l"},"source":["# 데이터 불러오기\n","from google.colab import drive\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/prepro_train.csv')\n","train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0IAlpe_28k9l"},"source":["test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/prepro_test.csv')\n","test.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cxE9nmop8k9m"},"source":["### 베이스라인 모델링\n","특정 기법을 통해 학습 및 평가했을 때, 기존의 모델보다 좋아졌는지 판단하기 위해 기준으로 삼을 베이스라인 모델을 생성한다."]},{"cell_type":"code","metadata":{"id":"6X9xu3__8k9n"},"source":["# 전처리한 변수들이 있는데 이 중에서 분석에 활용한 변수를 선택해줍시다.\n","# 기존 데이터에 덮어쓰기보단 train_copy라는 예비 데이터프레임을 생성하여 저장해줍시다.\n","train_copy = train\n","\n","col = ['holiday', 'workingday', 'atemp', 'humidity', 'windspeed',\n","       'rainyday', 'ideal', 'sticky', 'peak', 'temp(difference)',\n","       'discomfort_index', 'hour_0', 'hour_1', 'hour_2', 'hour_3', 'hour_5',\n","       'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12',\n","       'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18',\n","       'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23', 'hw_0', 'hw_1',\n","       'hw_2', 'hw_3', 'hw_5', 'hw_6', 'hw_7', 'hw_8', 'hw_9', 'hw_10',\n","       'hw_11', 'hw_12', 'hw_13', 'hw_14', 'hw_15', 'hw_16', 'hw_17', 'hw_18',\n","       'hw_19', 'hw_20', 'hw_21', 'hw_22', 'hw_23', 'year_2012', 'dayofweek_0',\n","       'dayofweek_2', 'dayofweek_3', 'dayofweek_4', 'dayofweek_5',\n","       'dayofweek_6', 'season_2', 'season_3', 'season_4', 'month_2', 'month_3',\n","       'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9',\n","       'month_10', 'month_11', 'month_12']\n","\n","# count를 제외한 변수들을 담은 데이터프레임.\n","X_features = train[col]\n","X_test = test[col]\n","\n","# 타겟 변수는 log 처리를 해준 count 변수\n","target = train['log_count']\n","\n","# 데이터를 나눠줌\n","X_train, X_valid, y_train, y_valid = train_test_split(X_features, target, test_size = 0.3, random_state = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AMJ7-cKT8k9n"},"source":["train.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vC6uzhDO8k9o"},"source":["# RMSLE 값을 출력하는 함수\n","def rmsle(y,y_,convertExp=True):\n","    # 지수화 필요하다면\n","    if convertExp:\n","        y = np.exp(y),\n","        y_ = np.exp(y_)\n","    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n","    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n","    calc = (log1 - log2) ** 2\n","    return np.sqrt(np.mean(calc))\n","\n","rmsle_scorer = make_scorer(rmsle)\n","rmsle_scorer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0G1JRdN8k9p"},"source":["## cross val score를 측정해주는 함수\n","def cv_score(models, n = 5):\n","    # kfold 수는 default인 5로 지정\n","    kf = KFold(n_splits = n, shuffle=True, random_state = 0)\n","    \n","    for model in models:\n","#       model.fit(X_train,y_train)\n","        score =  cross_val_score(model, X_features, target, cv = kf, scoring=rmsle_scorer)\n","        print(model[0],'의 평균 score:', round(score.mean(), 5))\n","        print(model[0],'의 std:', round(score.std(), 5))\n","        print()      \n","        \n","        # y_valid과 prediction을 비교하여 시각화 해주는 코드\n","#        g = sns.kdeplot(np.exp(y_valid),  color = 'skyblue', alpha = .6, fill = True, label = 'valid')\n","#        g = sns.kdeplot(np.exp(model.predict(X_valid)), color = 'orange', alpha = .3, fill = True, label = 'prediction')\n","#       plt.legend()\n","#        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TM6ZMe4N8k9q"},"source":["## 제출을 위한 함수\n","def submission(model):\n","    model.fit(X_features, target)\n","    prediction = np.exp(model.predict(X_test))\n","    \n","    # 자동으로 형식을 맞춰 csv 생성해주는 코드\n","    submission = pd.DataFrame(test['datetime'])\n","    submission['count'] = prediction\n","\n","    pd.DataFrame(submission).to_csv('submission_bike.csv', index = False)\n","    \n","    return pd.DataFrame(submission)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"UnDsz2xr8k9q"},"source":["from sklearn.linear_model import ElasticNet\n","# 기본 모델을 아래와 같이 5가지로 정했음.                                             mean val_score    std\n","pipe_lr = Pipeline([('model', LinearRegression())])                                 #   0.33503     / 0.00783\n","pipe_rf = Pipeline([('model', RandomForestRegressor(n_estimators=500))])            #   0.4394      / 0.01136 \n","pipe_lgbm = Pipeline([('model', LGBMRegressor(n_estimators=100))])                  #   0.32172     / 0.0055\n","pipe_gb = Pipeline([('model', GradientBoostingRegressor())])                        #   0.59616     / 0.01654\n","pipe_xgb = Pipeline([('model', XGBRegressor(objective ='reg:squarederror'))])       #   0.59705     / 0.0167\n","\n","models = [pipe_lr, pipe_rf, pipe_lgbm, pipe_gb, pipe_xgb]\n","\n","# 평균 valid score 측정\n","cv_score(models)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"P7WBB5348k9s"},"source":["# lr 모델 제출 결과 : => inf 값 나와서 실패.. 왜지...\n","# rf 모델 제출 결과 : 0.52784  **\n","# lgbm 모델 제출 결과 : 0.41477  **\n","# gb 모델 제출 결과 : 0.66858\n","# xgb 모델 제출 결과 : 0.52387  **\n","\n","submission(pipe_xgb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"37sj2GTY8k9s"},"source":["[RMSLE 평가 지표에 대해](https://ahnjg.tistory.com/90)\n","\n","요약 \n","1. 큰 것보다 적은 것을 오차없이 예측할 때 점수가 더 좋음.\n","2. under estimator에 대해 페널티를 부과한다. <b> 예측값 > 실제값</b> 보다 <b>예측값 < 실제값</b>일 때, 점수가 안 좋음."]},{"cell_type":"markdown","metadata":{"id":"L1lvpGmu8k9t"},"source":["## 파라미터 찾기"]},{"cell_type":"code","metadata":{"id":"RRtalzLcVKKq"},"source":["def search_params(x = X_features, y = target, model, paras, n = 5, scorer = rmsle_scorer) :\n","    # pipeline 으로 받은 모델을 부르고\n","    model = model['model']\n","\n","    # kfold \n","    kf = KFold(n_splits = n, shuffle=True, random_state = 0)\n","\n","    grid_model = GridSearchCV(estimator = model, param_grid = paras, cv=kf, n_jobs=-1, verbose=2, scoring = scorer)\n","    grid_model.fit(x,y)\n","    \n","    # grid_search한 결과를 Dataframe화\n","    scores_df = pd.DataFrame(grid_model.cv_results_)\n","    scores_df[['params', 'mean_test_score','rank_test_score', 'split0_test_score', 'split1_test_score','split2_test_score']]\n","    \n","    # GridSearchCV 의 best score는 높은 점수를 알려줌. \n","    # 즉, 현재 우리의 평가지표(rslme)는 낮은 점수일수록 좋은 평가이기 때문에 score를 오름차순으로 정렬해서 확인\n","    scores_df = scores_df.sort_values('mean_test_score', ascending = True).loc[:,['params','mean_test_score']]\n","\n","    return(scores_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0vQTZ1rVqBl"},"source":["#lgbm에 대한 parameters\n","para_lgbm = [{\n","    'learning_rate' : [0.01, 0.03, 0.05, 0.07, 0.1],\n","    'n_estimators' : [500, 800, 1000, 1300, 1500],\n","    'random_state' : [0]}]\n","    \n","search_params(X_features, target, pipe_lgbm, para_lgbm)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4uQdDraZOqW6"},"source":["1000/0.03 이 best score인 것을 확인할 수 있음"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"lUcDtSoH8k9u"},"source":["# lgbm (1500, 0.03, random_state = 0)\n","pipe_lgbm4 = Pipeline([('model', LGBMRegressor(n_estimators=1500, learning_rate = 0.03))])\n","\n","submission(pipe_lgbm4) # 0.399"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OswJVHO9Zieh"},"source":["cv_lgbm5 = Pipeline([('model', LGBMRegressor(n_estimators=1500, learning_rate = 0.03, random_state=0))])\n","model5 = [cv_lgbm5]\n","cv_score(model5)     \n","\n","#   파라미터 조정 후         파라미터 조정 전\n","# 0.28621  / 0.00548   =>    0.32172  / 0.0055"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RXAFx9M1aW9v"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"pO85Od3JX_Fl"},"source":["### *참고 ) 헤매는동안 살펴본 값들*\n"]},{"cell_type":"code","metadata":{"id":"l-4--XJGN3fW"},"source":["# grid_lgbm2 = LGBMRegressor()\n","# #\n","# kf = KFold(n_splits = 5, shuffle=True, random_state = 0)\n","\n","# grid_lgbm2 = GridSearchCV(estimator = grid_lgbm2, param_grid = para_lgbm, cv=kf, n_jobs=-1, verbose=2, scoring = rmsle_scorer)\n","# grid_lgbm2.fit(X_features, target)\n","\n","# #lgbm \n","# print(\"lgbm best param is : \", grid_lgbm2.best_params_)\n","# print(\"lgbm best score is : \", grid_lgbm2.best_score_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdWY0mRB8k9u"},"source":["# pipe_lgbm2 = Pipeline([('model', LGBMRegressor(n_estimators=500, learning_rate = 0.01, random_state = 0))])\n","\n","# submission(pipe_lgbm2) # 0.46298"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BuNHOUgO8k9v"},"source":["# pipe_lgbm3 = Pipeline([('model', LGBMRegressor(n_estimators=1000, learning_rate = 0.05, random_state = 0))])\n","# submission(pipe_lgbm3)  # 0.39642"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDdIZ0pKHJPh"},"source":["# pipe_lgbm4 = Pipeline([('model', LGBMRegressor(n_estimators=1000, learning_rate = 0.07, random_state = 0))])\n","# submission(pipe_lgbm4)   # 0.404"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rsCce9NQC4zF"},"source":["# cv_lgbm = Pipeline([('model', LGBMRegressor(n_estimators=500, learning_rate = 0.01))])\n","# model1 = [cv_lgbm]\n","# cv_score(model1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrnQtngsDsWh"},"source":["# cv_lgbm2 = Pipeline([('model', LGBMRegressor(n_estimators=1000, learning_rate = 0.05, random_state=0))])\n","# model2 = [cv_lgbm2]\n","# cv_score(model2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zl2TxzEIJgKW"},"source":["# cv_lgbm3 = Pipeline([('model', LGBMRegressor(n_estimators=1000, learning_rate = 0.07, random_state=0))])\n","# model3 = [cv_lgbm3]\n","# cv_score(model3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hya-6G7EH34H"},"source":["# scores_df.sort_values('mean_test_score', ascending = True).loc[:,['params','mean_test_score']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7V0NBT1GakVA"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"ZsP5ZRk8antW"},"source":["## 앞으로 찾아야할 파라미터값들"]},{"cell_type":"code","metadata":{"id":"l4-HY_Sm8k9v"},"source":["para_xgb = [{\n","    'eta' : [0.01, 0.05, 0.1, 0.15, 0.2], \n","    'gamma' : [0, 0.3, 0.5],\n","    'max_depth': [4, 6, 8],\n","    'random_state' : [0],\n","    'objective' : ['reg:squarederror']}]\n","\n","grid_xgb = XGBRegressor()\n","\n","\n","xgb_para = GridSearchCV(estimator = grid_xgb, param_grid = para_xgb, cv=10, n_jobs=-1, verbose=2, scoring = rmsle_scorer)\n","xgb_para.fit(X_features, target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"URxcHbit8k9v"},"source":["print(\"xgb best param is : \", xgb_para.best_params_)\n","print(\"xgb best score is : \", xgb_para.best_score_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6coGii8r8k9v"},"source":["pipe_xgb2 = Pipeline([('model', XGBRegressor(eta = 0.01, gamma= 0.3, max_depth= 4, objective= 'reg:squarederror', random_state= 0))])\n","submission(pipe_xgb2)  # 1.7656"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0AMNe_o-atwJ"},"source":["#rf에 대한 parameters\n","para_rf = [{\n","    'max_depth' : [6, 8, 10, 12, 14],\n","    'n_estimators' : [100,200,300,400,500],\n","    'min_samples_split' : [2, 5, 7,10],\n","    'min_samples_leaf' : [1, 2, 4],\n","    'random_state' : [0]}]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MouTmhFM8k9w"},"source":[""],"execution_count":null,"outputs":[]}]}