# -*- coding: utf-8 -*-
"""자전거수요예측_조하나_1207.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mcR3E8mRku_NSFacNCgQi-9R7XCQCE2g

### 라이브러리 및 데이터 불러오기
"""

# 기본 라이브러리
import numpy as np
import pandas as pd

# 시각화 라이브러리
import matplotlib.pyplot as plt 
import seaborn as sns
sns.set_style('darkgrid')

# 모델링을 위한 sklearn 패키지
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold #***
from sklearn.linear_model import RidgeCV, Ridge, LassoCV, Lasso, ElasticNetCV, ElasticNet #***
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,VotingRegressor #***^
from sklearn.linear_model import LinearRegression

# score를 내줄 함수
from sklearn import metrics
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_squared_log_error

# 모델링에 활용한 패키지
import lightgbm as lgbm 
from lightgbm import LGBMRegressor

import xgboost as xgb 
from xgboost import XGBRegressor #XGBRegressor

import warnings
# warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks (1)/content/preprocessed_train_2021-12-07.csv')
train.head()

test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks (1)/content/preprocessed_test_2021-12-02.csv')
test.tail()

"""### 베이스라인 모델링
특정 기법을 통해 학습 및 평가했을 때, 기존의 모델보다 좋아졌는지 판단하기 위해 기준으로 삼을 베이스라인 모델을 생성한다.
"""

# 전처리한 변수들이 있는데 이 중에서 분석에 활용한 변수를 선택해줍시다.
# 기존 데이터에 덮어쓰기보단 train_copy라는 예비 데이터프레임을 생성하여 저장해줍시다.
train_copy = train

col = ['holiday', 'workingday', 'atemp', 'humidity', 'windspeed',
       'rainyday', 'ideal', 'sticky', 'peak', 'temp(difference)',
       'discomfort_index', 'hour_0', 'hour_1', 'hour_2', 'hour_3', 'hour_5',
       'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12',
       'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18',
       'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23', 'hw_0', 'hw_1',
       'hw_2', 'hw_3', 'hw_5', 'hw_6', 'hw_7', 'hw_8', 'hw_9', 'hw_10',
       'hw_11', 'hw_12', 'hw_13', 'hw_14', 'hw_15', 'hw_16', 'hw_17', 'hw_18',
       'hw_19', 'hw_20', 'hw_21', 'hw_22', 'hw_23', 'year_2012', 'dayofweek_0',
       'dayofweek_2', 'dayofweek_3', 'dayofweek_4', 'dayofweek_5',
       'dayofweek_6', 'season_2', 'season_3', 'season_4', 'month_2', 'month_3',
       'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9',
       'month_10', 'month_11', 'month_12']

# count를 제외한 변수들을 담은 데이터프레임.
X_features = train[col]
X_test = test[col]

# 타겟 변수는 log 처리를 해준 count 변수
target = train['log_count']

# 데이터를 나눠줌
X_train, X_valid, y_train, y_valid = train_test_split(X_features, target, test_size = 0.3, random_state = 0)

train.columns

# RMSLE 값을 출력하는 함수
def rmsle(y,y_,convertExp=True):
    # 지수화 필요하다면
    if convertExp:
        y = np.exp(y),
        y_ = np.exp(y_)
    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))
    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))
    calc = (log1 - log2) ** 2
    return np.sqrt(np.mean(calc))

rmsle_scorer = make_scorer(rmsle)
rmsle_scorer

## cross val score를 측정해주는 함수
def cv_score(models, n = 5):
    # kfold 수는 default인 5로 지정
    kf = KFold(n_splits = n, shuffle=True, random_state = 0)
    
    for model in models:
#       model.fit(X_train,y_train)
        score =  cross_val_score(model, X_features, target, cv = kf, scoring=rmsle_scorer)
        print(model[0],'의 평균 score:', round(score.mean(), 5))
        print(model[0],'의 std:', round(score.std(), 5))
        print()      
        
        # y_valid과 prediction을 비교하여 시각화 해주는 코드
#        g = sns.kdeplot(np.exp(y_valid),  color = 'skyblue', alpha = .6, fill = True, label = 'valid')
#        g = sns.kdeplot(np.exp(model.predict(X_valid)), color = 'orange', alpha = .3, fill = True, label = 'prediction')
#       plt.legend()
#        plt.show()

## 제출을 위한 함수
def submission(model):
    model.fit(X_features, target)
    prediction = np.exp(model.predict(X_test))
    
    # 자동으로 형식을 맞춰 csv 생성해주는 코드
    submission = pd.DataFrame(test['datetime'])
    submission['count'] = prediction

    pd.DataFrame(submission).to_csv('submission_bike.csv', index = False)
    
    return pd.DataFrame(submission)

from sklearn.linear_model import ElasticNet
# 기본 모델을 아래와 같이 5가지로 정했음.                                             mean val_score    std
pipe_lr = Pipeline([('model', LinearRegression())])                                 #   0.33503     / 0.00783
pipe_rf = Pipeline([('model', RandomForestRegressor(n_estimators=500))])            #   0.4394      / 0.01136 
pipe_lgbm = Pipeline([('model', LGBMRegressor(n_estimators=100))])                  #   0.32172     / 0.0055
pipe_gb = Pipeline([('model', GradientBoostingRegressor())])                        #   0.59616     / 0.01654
pipe_xgb = Pipeline([('model', XGBRegressor(objective ='reg:squarederror'))])       #   0.59705     / 0.0167

models = [pipe_lr, pipe_rf, pipe_lgbm, pipe_gb, pipe_xgb]

# 평균 valid score 측정
cv_score(models)

# 보팅앙상블^
pipe_voting = Pipeline([('model', VotingRegressor(estimators=[('lr', LinearRegression()), ('rf', RandomForestRegressor(n_estimators=500))]))])   # mean val_score   std
models = [pipe_voting]                                                                                                                           #   0.33762      0.00791

# 평균 valid score 측정
cv_score(models)

# lr 모델 제출 결과 : => inf 값 나와서 실패.. 왜지...
# rf 모델 제출 결과 : 0.52784  **
# lgbm 모델 제출 결과 : 0.41477  **
# gb 모델 제출 결과 : 0.66858
# xgb 모델 제출 결과 : 0.52387  **
# voting 모델 제출 결과 : => inf 값 나와서 실패.. 왜지2... ^

submission(pipe_voting)

"""[RMSLE 평가 지표에 대해](https://ahnjg.tistory.com/90)

요약 
1. 큰 것보다 적은 것을 오차없이 예측할 때 점수가 더 좋음.
2. under estimator에 대해 페널티를 부과한다. <b> 예측값 > 실제값</b> 보다 <b>예측값 < 실제값</b>일 때, 점수가 안 좋음.

## 파라미터 찾기
"""

# grid_search - RandomForestRegressor^
param_grid={'n_estimators': range(100, 500, 100), 
            'max_features': ['auto', 'sqrt', 'log2']}
from sklearn.model_selection import GridSearchCV
grid_search=GridSearchCV(RandomForestRegressor(), param_grid, cv=5)
grid_search.fit(X_features, target)

print("Best Parameter: {}".format(grid_search.best_params_))
print("Best Score: {:.4f}".format(grid_search.best_score_))

# Best Parameter: {'max_features': 'auto', 'n_estimators': 400}
# Best Score: 0.8092

# Random_search - RandomForestRegressor^
from scipy.stats import randint
param_distribs = {'n_estimators': randint(low=100, high=500), 
                  'max_features': ['auto', 'sqrt', 'log2']}
from sklearn.model_selection import RandomizedSearchCV
random_search=RandomizedSearchCV(RandomForestRegressor(), 
                                 param_distributions=param_distribs, n_iter=20, cv=5)
random_search.fit(X_features, target)


print("Best Parameter: {}".format(random_search.best_params_))
print("Best Score: {:.4f}".format(random_search.best_score_))

# Best Parameter: {'max_features': 'auto', 'n_estimators': 365}
# Best Score: 0.8107

def search_params(x, y, model, paras, n = 5, scorer = rmsle_scorer) :
    # pipeline 으로 받은 모델을 부르고
    model = model['model']

    # kfold 
    kf = KFold(n_splits = n, shuffle=True, random_state = 0)

    grid_model = GridSearchCV(estimator = model, param_grid = paras, cv=kf, n_jobs=-1, verbose=2, scoring = scorer)
    grid_model.fit(x,y)
    
    # grid_search한 결과를 Dataframe화
    scores_df = pd.DataFrame(grid_model.cv_results_)
    scores_df[['params', 'mean_test_score','rank_test_score', 'split0_test_score', 'split1_test_score','split2_test_score']]
    
    # GridSearchCV 의 best score는 높은 점수를 알려줌. 
    # 즉, 현재 우리의 평가지표(rslme)는 낮은 점수일수록 좋은 평가이기 때문에 score를 오름차순으로 정렬해서 확인
    scores_df = scores_df.sort_values('mean_test_score', ascending = True).loc[:,['params','mean_test_score']] 
    # 낮은 순으로 정렬: 가장 맨 위에 있는게 ,점수가 제일 좋은 점수가 올라오니까, 그게 베스트 파라미터다.

    return(scores_df)

#lgbm에 대한 parameters
para_lgbm = [{
    'learning_rate' : [0.01, 0.03, 0.05, 0.07, 0.1],
    'n_estimators' : [500, 800, 1000, 1300, 1500],
    'random_state' : [0]}]
    
search_params(X_features, target, pipe_lgbm, para_lgbm)

"""1000/0.03 이 best score인 것을 확인할 수 있음"""

# lgbm (1500, 0.03, random_state = 0)
pipe_lgbm4 = Pipeline([('model', LGBMRegressor(n_estimators=1500, learning_rate = 0.03))])

submission(pipe_lgbm4) # 0.399

cv_lgbm5 = Pipeline([('model', LGBMRegressor(n_estimators=1500, learning_rate = 0.03, random_state=0))])
model5 = [cv_lgbm5]
cv_score(model5)     

#   파라미터 조정 후         파라미터 조정 전
# 0.28621  / 0.00548   =>    0.32172  / 0.0055







